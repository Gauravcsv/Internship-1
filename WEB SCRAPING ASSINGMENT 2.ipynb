{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5700e2d0",
   "metadata": {},
   "source": [
    "# Question 1-: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps: \n",
    "\n",
    "1-First get the webpage https://www.naukri.com/\n",
    "2-Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3-Then click the search button.\n",
    "4-Then scrape the data for the first 10 jobs results you get.\n",
    "5-Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eded6167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\gaurav panwar\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\charomedriver.exe')\n",
    "\n",
    "driver = webdriver.Chrome('charomdriver.exe')\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()\n",
    "\n",
    "url1=\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\"\n",
    "\n",
    "driver.get(url1)\n",
    "\n",
    "\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "experiences_list=[]\n",
    "\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:4]\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "\n",
    "\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:4]\n",
    "\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:4]\n",
    "\n",
    "\n",
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:4]\n",
    "\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    locations_list.append(locations)\n",
    "locations_list[0:4]\n",
    "\n",
    "\n",
    "experiences_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experiences_tags[0:4]\n",
    "\n",
    "for i in experiences_tags:\n",
    "    experience=i.text\n",
    "    experiences_list.append(experience)\n",
    "experiences_list[0:4]\n",
    "\n",
    "\n",
    "(len(job_titles),len(company_names),len(locations_list),len(experiences_list))\n",
    "\n",
    "\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Job_title']=job_titles[0:10]\n",
    "jobs['Company_name']=company_names[0:10]\n",
    "jobs['Job_location']=locations_list[0:10]\n",
    "jobs['Experience_required']=experiences_list[0:10]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2414a",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "    \n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore/Bengaluru\")\n",
    "\n",
    "\n",
    "\n",
    "url2=\"https://www.naukri.com/data-scientist-jobs-in-bangalore-bengaluru?k=data%20scientist&l=bangalore%2Fbengaluru\"\n",
    "\n",
    "driver.get(url2)\n",
    "\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:4]\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:4]\n",
    "\n",
    "\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:4]\n",
    "\n",
    "\n",
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:4]\n",
    "\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    locations_list.append(locations)\n",
    "locations_list[0:4]\n",
    "\n",
    "(len(job_titles),len(company_names),len(locations_list))\n",
    "\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job_title']=job_titles[0:10]\n",
    "jobs['company_name']=company_names[0:10]\n",
    "jobs['job_location']=locations_list[0:10]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610916df",
   "metadata": {},
   "source": [
    "# Question 3-: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "1-first get the webpage https://www.naukri.com/\n",
    "2-Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3-Then click the search button.\n",
    "4-Then apply the location filter and salary filter by checking the respective boxes\n",
    "5-Then scrape the data for the first 10 jobs results you get.\n",
    "6-Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6217af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Delhi\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()\n",
    "\n",
    "select_location = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/p/span[1]')\n",
    "select_location.click()\n",
    "\n",
    "select_salary = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/p/span[1]')\n",
    "select_salary.click()\n",
    "\n",
    "url3=\"https://www.naukri.com/data-scientist-jobs-in-delhi?k=data%20scientist&l=delhi&cityTypeGid=9508&ctcFilter=3to6\"\n",
    "\n",
    "driver.get(url3)\n",
    "\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:4]\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:4]\n",
    "\n",
    "\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:4]\n",
    "\n",
    "\n",
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:4]\n",
    "\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    locations_list.append(locations)\n",
    "locations_list[0:4]\n",
    "\n",
    "(len(job_titles),len(company_names),len(locations_list))\n",
    "\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['titles']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['location']=locations_list[0:10]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1059e11",
   "metadata": {},
   "source": [
    "# Quation 4-  Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "1-Brand\n",
    "2-Product Description\n",
    "3-Price\n",
    "4-Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c471bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "login_pop.click()\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sunglasses\")\n",
    "search_job.submit()\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "    \n",
    "Brand_name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_name.append(i.text)\n",
    "    \n",
    "try:\n",
    "    description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "except NoSuchElementException as e:\n",
    "    description_tags=' '\n",
    "    \n",
    "Product_Description=[]\n",
    "for i in description_tags:\n",
    "    Product_Description.append(i.text)\n",
    "    \n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "    \n",
    "Product_Price=[]\n",
    "for i in price_tags:\n",
    "    Product_Price.append(i.text)\n",
    "    \n",
    "import pandas as pd\n",
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']=Brand_name[0:100]\n",
    "Sunglasses['Description']=Product_Description[0:100]\n",
    "Sunglasses['Price']=Product_Price[0:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa0568",
   "metadata": {},
   "source": [
    "# Question 5- Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'    \n",
    "    driver.get(url)\n",
    "    try:\n",
    "        rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    except NoSuchElementException as e:\n",
    "        rating_tags='-'\n",
    "        \n",
    "    Ratings=[]\n",
    "    for i in rating_tags:\n",
    "        Ratings.append(i.text)\n",
    "    \n",
    "    try:\n",
    "        summary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    except NoSuchElementException as e:\n",
    "        summary_tags='-'\n",
    "        \n",
    "    Summary=[]\n",
    "    for i in summary_tags:\n",
    "        Summary.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        FullReview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']//div//div\")\n",
    "    except NoSuchElementException as e:\n",
    "        FullReview_tags='-'\n",
    "        \n",
    "    FullSummary=[]\n",
    "    for i in FullReview_tags:\n",
    "        FullSummary.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "IPhone=pd.DataFrame({})\n",
    "IPhone['Ratings']=Ratings[0:100]\n",
    "IPhone['Summary']=Summary[0:100]\n",
    "IPhone['FullSummary']=FullSummary[0:100]\n",
    "IPhone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625535b2",
   "metadata": {},
   "source": [
    "# Question 6-: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "# Here .click function use to tap on desire elements of webpage\n",
    "login_pop.click()\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sneakers\")\n",
    "search_job.submit()\n",
    "\n",
    "\n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    except NoSuchElementException as e:\n",
    "        brand_tags='-'\n",
    "        \n",
    "    BrandTags=[]\n",
    "    for i in brand_tags:\n",
    "        BrandTags.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        ProductDesc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    except NoSuchElementException as e:\n",
    "        ProductDesc_tags='-'\n",
    "        \n",
    "    ProductDesc=[]\n",
    "    for i in ProductDesc_tags:\n",
    "        ProductDesc.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    except NoSuchElementException as e:\n",
    "        Price_tags='-'\n",
    "        \n",
    "    Price=[]\n",
    "    for i in Price_tags:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        Discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    except NoSuchElementException as e:\n",
    "        Discount_tags='-'\n",
    "        \n",
    "    Discount=[]\n",
    "    for i in Discount_tags:\n",
    "        Discount.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=BrandTags[0:30]\n",
    "sneakers['Shoe Description']=ProductDesc[0:30]\n",
    "sneakers['Price']=Price[0:30]\n",
    "sneakers['Discount']=Discount[0:30]\n",
    "sneakers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642b545",
   "metadata": {},
   "source": [
    "# Quesion 7-: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94621de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "price=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price.click()\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Color filter to “Black”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82adfbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "color=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "color.click()\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929530e4",
   "metadata": {},
   "source": [
    "# Question 8-: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "Url=\"https://www.amazon.in/\"\n",
    "\n",
    " \n",
    "driver.get(Url)\n",
    "\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"nav-search-field \")\n",
    "print(len(inputboxs))\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\").send_keys(\"Laptop\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "Intel_Core_i7 = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[2]/li[16]/span\")\n",
    "try:\n",
    "    Intel_Core_i7.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(Intel_Core_i7.get_attribute(\"herf\"))\n",
    "\n",
    "time.sleep(4) \n",
    "\n",
    "Intel_Core_i9 = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[18]/span/a/span\")\n",
    "try:\n",
    "    Intel_Core_i9.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(Intel_Core_i9.get_attribute(\"herf\"))\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "Laptop_Brand = [] \n",
    "Ratings = []\n",
    "Price = []\n",
    "import time\n",
    "\n",
    "for j in range(0,5,1):\n",
    "    Laptop_Brand_tag=driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]') \n",
    "    for i in Laptop_Brand_tag:\n",
    "        Laptop_Brand.append(i.text)\n",
    "          \n",
    "    Ratings_tag=driver.find_elements_by_xpath('//span[@class=\"a-icon-alt\"]') \n",
    "    for i in Ratings_tag:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "        \n",
    "    Price_tag=driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]') \n",
    "    for i in Price_tag:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "        \n",
    "    driver.find_element_by_xpath('//ul[@class=\"a-pagination\"]').click()\n",
    "    \n",
    "    time.sleep(4)\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "laptop_ws = pd.DataFrame({})\n",
    "laptop_ws[\"Title\"]=Laptop_Brand[0:50]\n",
    "laptop_ws[\"Ratings\"]=Ratings[0:50]\n",
    "laptop_ws[\"Price\"]=Price[0:50]\n",
    "\n",
    "\n",
    "\n",
    "laptop_ws.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a57cb6",
   "metadata": {},
   "source": [
    "# Question 9-: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "driver9= webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "driver9.get(\"https://www.ambitionbox.com/\")\n",
    "jobs= driver9.find_element_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[6]')\n",
    "jobs.click()\n",
    "\n",
    "search= driver9.find_element_by_xpath('//*[@id=\"jobs-typeahead\"]/span/input')\n",
    "search.send_keys('data scientist')\n",
    "\n",
    "search_btn= driver9.find_element_by_xpath('//*[@id=\"jobSearchPage\"]/div[2]/div[1]/div/div/div/button')\n",
    "search_btn.click()\n",
    "\n",
    "location= driver9.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[1]/i').click()\n",
    "\n",
    "search_location= driver9.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "search_location.send_keys(\"Noida\")\n",
    "\n",
    "noida_location= driver9.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[3]/div/div/div[1]/label')\n",
    "noida_location.click()\n",
    "\n",
    "company_name=[]\n",
    "time=[]\n",
    "rating=[]\n",
    "\n",
    "company = driver9.find_elements_by_xpath('//div[@class=\"company-info\"]')\n",
    "for i in company:\n",
    "    company_name.append(i.text)\n",
    "company_name[0:3]\n",
    "\n",
    "timing= driver9.find_elements_by_xpath('//span[@class=\"body-small-l\"]')\n",
    "for i in timing:\n",
    "    time.append(i.text)\n",
    "time[0:3]\n",
    "\n",
    "rate= driver9.find_elements_by_xpath('//span[@class=\"body-small\"]')\n",
    "for i in rate:\n",
    "    rating.append(i.text)\n",
    "rating[0:3]\n",
    "\n",
    "ambitionbox_data=pd.DataFrame({})\n",
    "ambitionbox_data[\"Comany_name\"]=company_name[0:10]\n",
    "ambitionbox_data[\"Posted_Before\"]=time[0:10]\n",
    "ambitionbox_data[\"Rating\"]=rating[0:10]\n",
    "ambitionbox_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504d609",
   "metadata": {},
   "source": [
    "# Question 10-: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver10= webdriver.Chrome(r\"C:\\Users\\Gaurav Panwar\\Downloads\\chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "driver10.get(\"https://www.ambitionbox.com/\")\n",
    "\n",
    "salaries_tab = driver10.find_element_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[4]')\n",
    "salaries_tab.click()\n",
    "\n",
    "search_job_profile = driver10.find_element_by_xpath('//*[@id=\"jobProfileSearchbox\"]')\n",
    "search_job_profile.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_datascientist= driver10.find_element_by_xpath('/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]')\n",
    "search_datascientist.click()\n",
    "\n",
    "company_name=[]\n",
    "salary=[]\n",
    "avg_salary=[]\n",
    "min_salary=[]\n",
    "max_salary=[]\n",
    "exp_required=[]\n",
    "company= driver10.find_elements_by_xpath('//div[@class=\"name\"]/a')\n",
    "for i in company:\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "company_name\n",
    "\n",
    "salaries= driver10.find_elements_by_xpath('//div[@class=\"name\"]/span')\n",
    "for i in salaries:\n",
    "    salary.append(i.text)\n",
    "salary\n",
    "\n",
    "avg= driver10.find_elements_by_xpath('//div[@class=\"average-indicator-wrapper\"]')\n",
    "for i in avg:\n",
    "    avg_salary.append(i.text)\n",
    "avg_salary[0:10]\n",
    "\n",
    "min= driver10.find_elements_by_xpath('//div[@class=\"value body-medium\"]')\n",
    "for i in range(0,len(min),2):\n",
    "    min_salary.append(min[i].text)\n",
    "min_salary[0:10]\n",
    "\n",
    "max= driver10.find_elements_by_xpath('//div[@class=\"value body-medium\"]')\n",
    "for i in range(1,len(max),2):\n",
    "    max_salary.append(max[i].text)\n",
    "max_salary[0:10]\n",
    "\n",
    "exp= driver10.find_elements_by_xpath('//div[@class=\"salaries sbold-list-header\"]')\n",
    "for i in exp:\n",
    "    exp_required.append(i.text.replace(\"\\n\",\"\"))\n",
    "exp_required[0:10]\n",
    "\n",
    "print(len(company_name),len(avg_salary),len(min_salary),len(max_salary),len(exp_required))\n",
    "\n",
    "ambitionbox_dataset=pd.DataFrame({})\n",
    "ambitionbox_dataset[\"Company_Name\"]=company_name[0:10]\n",
    "ambitionbox_dataset[\"Total_Salary\"]=salary[0:10]\n",
    "ambitionbox_dataset[\"Avg_Salary\"]=avg_salary[0:10]\n",
    "ambitionbox_dataset[\"Min_Salary\"]=min_salary[0:10]\n",
    "ambitionbox_dataset[\"Max_Salary\"]=max_salary[0:10]\n",
    "ambitionbox_dataset[\"Total_Experience\"]=exp_required[0:10]\n",
    "ambitionbox_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
